[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Short Stories on Machine Learning\n",
    "section": "",
    "text": "These pages are a collection of notes I’ve written mostly for my own sake to help me remember subtle arguments or how to think about concepts that constantly confuse me. In the hopes that maybe my way of thinking resonates with others, I’ve decided to share these notes here.\nPlease feel free to reach out if you want to connect on these topics."
  },
  {
    "objectID": "blog/hyperparameter_optimization.html",
    "href": "blog/hyperparameter_optimization.html",
    "title": "Hyperparameter Optimization and Propensity Score Matching",
    "section": "",
    "text": "Despite the intimidating names given to methods like tree-structured Parzen estimation, efficient hyperparameter optimization doesn’t need to be complicated."
  },
  {
    "objectID": "blog/hyperparameter_optimization.html#introduction",
    "href": "blog/hyperparameter_optimization.html#introduction",
    "title": "Hyperparameter Optimization and Propensity Score Matching",
    "section": "Introduction",
    "text": "Introduction\n\nHyperparameter optimization\nThis post came about as a rebellion against obfuscation. I was trying to optimize the hyperparameters of a classification model and came across an interesting-looking paper [1]. Unfortunately, the paper is dense with mathematics and gave me no intuitive insight into what’s happening. As I dug deeper, I found that the ideas they were presenting are absurdly simple (thanks partly to the post in [2]) and I wanted to capture this simplicity for future reference.\nI will do this in the actual setting in which my use case arose: propensity score (PS) matching. There are some prerequisites for this discussion, which I unfortunately have to assume. If you aren’t familiar, the classic reference on PS matching is [3], although I find the more recent review in [4] to be more accessible. In any case, suffice it to say that in PS matching, one trains a binary classification model and needs somehow to choose the “best” possible model over a large space of modeling choices.\nI refer to these modeling choices as hyperparameters. For me, a hyperparameter is any decision you make along the way to making a prediction with a model that cannot be optimized by gradient descent. I like this definition because it includes preprocessing operations that often aren’t thought of as hyperparameters. For instance, do you subtract the mean from your variables before optimizing the model parameters? This choice is a hyperparameter: subtract_mean \\(\\in \\{True, False\\}\\). In many cases, there is conventional wisdom around certain choices and these choices have a well-chosen default, but the choice is there, it affects your analysis, it can’t be optimized by gradient descent and it’s therefore by my definition a hyperparameter.\nMore conventional hyperparameters are quantities like regularization strength (which was discussed in a previous post ), type of regularization (e.g., L1 versus L2, discussed in the same post), maximum tree depth for a tree-based model, number of estimators in an ensemble method, number of layers in a multi-layer perceptron, and even the choice of which model to use. A bad choice for hyperparameters can lead to poor performance in your model, so it’s worthwhile to spend time thinking about how one can best optimize an objective function over these hyperparameters.\n\n\nPropensity score matching\nWe want to answer the question: does treatment X cause outcome Y? Although the ideal approach to answering such a question is the randomized control trial, it’s often very impractical to perform such a trial; meanwhile there exists a wealth of high quality data that potentially already contain the cause-effect signal we’re interested in, for instance, electronic health record data generated through routine medical practice.\nWhen the intervention is not randomly assigned, there will often be systematic differences between treatment groups, which may obscure the causal signal if not taken into account. It turns out that by “matching” patients on their probability of treatment assignment, referred to as their propensity score, one can generate groups of patients with identical distributions of baseline covariates. It follows then that one can compare that outcomes in these matched groups and obtain an unbiased estimate of the treatment effect. For details, see [3] and [4].\nThis all works in theory, when the propensity score is known. However, the propensity score is never known in observational settings and must be estimated from the data. The estimation of the propensity score from the data requires modeling choices and we want to somehow choose the “best” propensity score model over a large space of possible models.\nWe define the following procedure for optimizing the hyperparameters of the propensity score model: \n\nChoose a set of hyperparameters \nTrain a model to predict P(treat|covariates) \nPerform matching on the estimated propensity score \nEvaluate the achieved covariate balance \n\nOur focus will be on how to implement step (1) such that we arrive quickly at a good set of hyperparameters, since steps (2) and (3) can be quite time-consuming. For (4), we compute the area between the 1-dimensional marginal distributions as a measure of balance to be optimized."
  },
  {
    "objectID": "blog/hyperparameter_optimization.html#simulation-setup",
    "href": "blog/hyperparameter_optimization.html#simulation-setup",
    "title": "Hyperparameter Optimization and Propensity Score Matching",
    "section": "Simulation Setup",
    "text": "Simulation Setup\nTo start, we’ll simulate a toy dataset consisting of two covariates. To make the example more tangible, we’ll give the covariates names. One will be BMI, the other will be eGFR. We’re interested in studying the effect of a certain medication on a patient population as seen in electronic health record databases. The problem: both BMI and eGFR influence the treatment decision by practicing physicians. The medication is not prescribed to patients that are extremely overweight, nor to patients that are extremely underweight. Furthermore, the medication is not prescribed to patients with poor kidney function (low eGFR), nor is it prescribed to patients with normal kidney function (high eGFR). As a result of the interaction between these covariates and the treatment decision, we cannot simply compared treated and untreated patients. We have to correct for these systematic differences through matching.\nBelow we show the distribution of patients and the simulated propensity score. The propensity score shown here represents the “true” (unknown) propensity score that generates the treatment decision. Our goal here is to estimate this quantity.\n\n\nCode\ndef make_bmi_egfr_blob(n_samples: int) -&gt; pd.DataFrame:\n    \n    # eGFR has a mean of 55 and a spread of +/- 40; clip to range [5, 120]\n    # This clipping is actually common as often measurements just report &gt;= 120\n    egfr = 55 + 40 * np.random.power(1, n_samples) * np.random.choice([-1, 1], size=n_samples)\n    egfr = np.clip(egfr, 5, 120)\n    \n    # BMI has mean 20 and a spread +/- 10; clip to range [2, 80]\n    bmi = 20 + 10 * np.random.randn(n_samples)\n    bmi = np.clip(bmi, 2, 80)\n\n    features = pd.DataFrame.from_records(\n        np.vstack((egfr, bmi)).T,\n        columns=['eGFR', 'BMI']\n    )\n    \n    return features\n\ndef ps_true(x, y):\n    # propensity score is a circle of radius == standard deviation around the mean\n    x_ = (x - x.mean()) / x.std()\n    y_ = (y - y.mean()) / y.std()\n    return expit( - (x_**2 + y_**2 ) )\n\n\n\n\n\n\n\nFigure 1: Scatter plot showing the BMI and eGFR values of the simulated patient population. Color indicates the true (and unknown) probability of treatment.\n\n\n\n\nGiven this form for the propensity score, we can now sample the realized treatment group assignment. In doing so, we get the distributions shown below for BMI and eGFR for the treated and untreated groups. We see that there are significant differences between the treated and untreated group other than the treatment. It would therefore not be fair to directly compare the health outcomes of the groups and then attribute differences in outcomes to the treatment.\n\n\nCode\ntreat_indicator = (np.random.rand(n_samples) &lt;= propensity_score).astype(int)\ntreat_label = ['treated' if t else 'untreated' for t in treat_indicator]\n\n\n\n\n\n\n\nFigure 2: Input distributions to the matching problem.\n\n\n\n\nThe number of patients in each group is given as below. The goal will be to choose a subset of patients from the untreated group such that distributions of BMI and eGFR in the chosen subset match the distributions in the treated group.\n\n\n\n\n\n\n\n\n\nN\n\n\ntreat\n\n\n\n\n\nuntreated\n8024\n\n\ntreated\n1976"
  },
  {
    "objectID": "blog/hyperparameter_optimization.html#matching-on-the-true-propensity-score",
    "href": "blog/hyperparameter_optimization.html#matching-on-the-true-propensity-score",
    "title": "Hyperparameter Optimization and Propensity Score Matching",
    "section": "Matching on the true propensity score",
    "text": "Matching on the true propensity score\nIn practice, we never have direct knowledge of the propensity score. But since we do in our simulation, let’s first check what happens when we match using the true propensity score. Namely, for each patient in the treated group, we select a corresponding patient from the untreated group with a similar propensity score. The resulting population is denoted by the label “matched” in the figure below. This solution represents the best we can hope to achieve with propensity score matching.\n\n\n\n\n\nFigure 3: Result of matching using the true propensity score.\n\n\n\n\nAs you can see, propensity score matching delivers on its promise – the covariates distributions of the population matched to the treated population on the propensity score (green) are nearly identical to that of the treated population (blue)."
  },
  {
    "objectID": "blog/hyperparameter_optimization.html#matching-on-estimated-propensity-score",
    "href": "blog/hyperparameter_optimization.html#matching-on-estimated-propensity-score",
    "title": "Hyperparameter Optimization and Propensity Score Matching",
    "section": "Matching on estimated propensity score",
    "text": "Matching on estimated propensity score\nNow let’s see how well we can perform matching by estimating the propensity score. To start, let’s try using a logistic regression to estimate the propensity score. As the logistic regression has linear decision boundaries, it should be clear that such a model would not be expected to perform well on this particular data, and indeed that is what we see.\n\n\n\n\n\nFigure 4: Result of using logistic regression to model propensity score.\n\n\n\n\nAs seen in the left hand plot above, the model estimates the true PS very poorly. As a result, when we match using the estimated PS, we get very poor balance, as seen below. Although the distribution on BMI looks somewhat balanced, eGFR in the matched population is left completely unbalanced. This result is simply a consequence of our modeling choice, not a breakdown of the PS approach, since we know from above that matching works when we know the true PS.\n\n\n\n\n\nFigure 5: Achieved balance of PS matching using logistic regression model for PS.\n\n\n\n\nSince it’s clear that the propensity score has some non-linearity to it, it make sense to next try using some sort of model that can capture this non-linearity. Let’s try therefore a random forest model for the propensity score. To illustrate the point, I will purposely pick very poor hyperparameters for the random forest: max_depth=2, n_estimators=2. The resulting estimation for PS is shown below. Although the estimation is still quite poor, the random forest does seem to better model the PS than the logistic regression.\n\n\n\n\n\nFigure 6: Result of using random forest to model propensity score.\n\n\n\n\nThe poor performance of this choice of hyperparameters is also reflected in the resulting matched distributions, shown below. Although better than the logistic regression, it’s clear that the matching is far from the ideal case of matching on the true PS. So we have some work left.\n\n\n\n\n\nFigure 7: Achieved balance of PS matching using random forest model for PS."
  },
  {
    "objectID": "blog/hyperparameter_optimization.html#hyperparameter-optimization-1",
    "href": "blog/hyperparameter_optimization.html#hyperparameter-optimization-1",
    "title": "Hyperparameter Optimization and Propensity Score Matching",
    "section": "Hyperparameter optimization",
    "text": "Hyperparameter optimization\n\nRandom search\nWe’ve seen that the choice of hyperparameters makes a difference in the achieved balance. So let’s try a few more choices of hyperparameters and see if we can find some improvements in the balance. For the logistic regression, we randomly sample a few values for the regularization strength \\(C\\); for the random forest model, we randomly sample a few values for the max_depth, keeping the n_estimators fixed to 2, as before. The resulting balance (roughly, the total area between the 1D marginal distributions) as a function of these hyperparameters is shown below.\n\n\n\n\n\nFigure 8: Achieved balance of various models for PS.\n\n\n\n\nAfter trying a few values of hyperparameters, we’ve realized that training models takes a long time and we can only try a limited number of hyperparameters. If given only one more chance, which model would you pick? So far it looks like the random forest model is the better bet and furthermore, it might be useful to try a max depth between 5 and 15, as this seems to be the region of hyperparameter space where the balance is the best.\nThis simple calculation, which you just did in your brain instinctively, is really all there is to hyperparameter optimization by tree-structured Parzen estimation (TPE), at least conceptually. For each model (logistic regression or random forest), we construct a “hypermodel” which predicts as a function of the various hyperparameters the expected balance. Our brains created these hypermodels automatically. If you proceed to make these models formal, then you’ll arrive eventually at TPE.\n\n\nTree-structured Parzen estimator\nSo, now that we understand what TPE is doing at a very high level, let’s see if we can use it to achieve even better balance. We will use the hyperopt library. To make things a little more interesting, we’ll optimize not just over model choice (LogisticRegression or RandomForest), \\(C\\) and max_depth, but also over the penalty type (L1 or L2) for logistic regression, n_estimators and min_samples_leaf for random forest (see here for a description of these parameters). After running the TPE sampler for 1000 iterations, the code returns the best parameters found, shown below.\n\n\nCode\n# define a search space\nfrom hyperopt import hp\nfrom hyperopt import fmin, tpe, space_eval, Trials\n\nspace = hp.choice('model', [\n    (LogisticRegression, {\n        'C':hp.loguniform('C', -4.6, 4.6), # BASE E\n        'penalty':hp.choice('penalty', ['l1', 'l2']),\n        'solver':'saga'\n    }),\n    (RandomForestClassifier, {\n        'n_estimators':hp.uniformint('n_estimators', 2, 100),\n        'min_samples_leaf':hp.uniformint('min_samples_leaf', 1, 500),\n        'max_depth':hp.uniformint('max_depth', 2, 30),\n        'random_state':123\n    })\n])\n\n# minimize the objective over the space\ntrials = Trials()\nbest = fmin(ps_match, space, algo=tpe.suggest, max_evals=1000, trials=trials, verbose=False)\n\nprint('Best model found:')\nfor param in best.items():\n    print(f'\\t{param[0]:20}{param[1]}')\n    \n\n\nBest model found:\n    max_depth           24.0\n    min_samples_leaf    211.0\n    model               1\n    n_estimators        6.0\n\n\nLet’s take a look at what the sampler did to arrive at this result. Below, I show how the sampling of the hyperparameter space evolves over time using the TPE method.\n\n\n\n\n\nFigure 9: An illustration of how the sampling of hyperparameters varies over time in TPE for the random forest hyperparameters.\n\n\n\n\nWe see a number of interesting features. In the top frame, we see that the sampler choose the logistic regression (LR) model less frequently over time; this is consistent with what we saw above, namely that the random forest model seems to perform better. The hypermodels of the TPE pick up on this fact and use it to preferentially choose the model that is showing the best results.\nWe also see that the sampler explores the space rather uniformly in the beginning (first ~200 iterations; this is tunable). The sampler is doing this because it needs something to train its hypermodels on. Once it has some data to work with, the sampler can start taking more educated guesses about where to explore next. We see this in the time series of the samples for max_depth, n_estimators and min_samples_leaf. For later times, the sampler almost never chooses max_depth &lt; 10. It seems to have figured out that this choice doesn’t have a good chance of working out. Similarly, the sampler stops choosing n_estimators &gt; 50 after a while.\nNote that the sampler never completely stops exploring any region of parameter space. Especially as the number of hyperparameter dimensions grows large, there will always be unexplored regions of parameter space. Therefore, this occasional random exploration of the space, even when a good objective value seems unlikely, is useful to avoid missing out.\nFinally, let’s see what the matching results look like! Below, I show the resulting PS estimation using the best values for the hyperparameters found by the TPE sampler. Although the results are not perfect, we have clearly greatly improved upon the initial results.\n\n\n\n\n\nFigure 10: Result of using TPE to find best hyperparameters to model propensity score.\n\n\n\n\nFurthermore, as shown below, the resulting matched populations look great! I would argue this match is hard to distinguish from the match above using the true propensity score.\n\n\n\n\n\nFigure 11: Achieved balance of PS matching using best model achieved from TPE."
  },
  {
    "objectID": "blog/hyperparameter_optimization.html#conclusion",
    "href": "blog/hyperparameter_optimization.html#conclusion",
    "title": "Hyperparameter Optimization and Propensity Score Matching",
    "section": "Conclusion",
    "text": "Conclusion\nHyperparameter optimzation is an important step in any machine learning workflow. Random sampling of hyperparameters is a very inefficient way to optimize hyperparameters, as our eyes can very quickly tell us. Tree-structured Parzen estimation is a nice alternative to random hyperparameter search, with easy-to-use Python implementations readily available. Although the method sounds intimidating, it’s really nothing more than a formalization of what we would naturally do anyway if we were to manually choose hyperparameters to test."
  },
  {
    "objectID": "blog/hyperparameter_optimization.html#further-reading",
    "href": "blog/hyperparameter_optimization.html#further-reading",
    "title": "Hyperparameter Optimization and Propensity Score Matching",
    "section": "Further Reading",
    "text": "Further Reading\n[1] Algorithms for Hyper-Parameter Optimization.\n[2] Building a Tree-Structured Parzen Estimator from Scratch(Kind Of).\n[3] The Central Role of the Propensity Score in Observational Studies for Causal Effects.\n[4] An Introduction to Propensity Score Methods for Reducing the Effects of Confounding in Observational Studies.\n[5] Hyperopt (Python library)."
  },
  {
    "objectID": "blog/regularization.html",
    "href": "blog/regularization.html",
    "title": "Regularization as a Feature Selection Method",
    "section": "",
    "text": "Often thought of as a method to prevent overfitting, regularization can also be used to select the most important features from a model."
  },
  {
    "objectID": "blog/regularization.html#what-is-regularization",
    "href": "blog/regularization.html#what-is-regularization",
    "title": "Regularization as a Feature Selection Method",
    "section": "What is regularization?",
    "text": "What is regularization?\nIn any machine learning setting, you invariably find yourself with a set of parameters \\(\\vec{\\beta}\\), which you wish to estimate from the data \\(\\mathcal{D}\\). A standard approach to estimating \\(\\vec{\\beta}\\) is to write down a likelihood function \\(P(\\mathcal{D}|\\mathbf{\\beta})\\) and maximize the likelihood with respect to \\(\\vec{\\beta}\\). This approach is called maximum likelihood estimation.\nWith regularization, we shift from a maximum likelihood estimation to a maximum a posterior estimation. In the Bayes’ formulation, we have the posterior equation:\n\\[ P(\\vec{\\beta} | \\mathcal{D}) = \\frac{P(\\mathcal{D}|\\vec{\\beta}) P(\\vec{\\beta})}{P(\\mathcal{D})}, \\]\nwhere the term \\(P(\\vec{\\beta})\\) is called the prior. It represents all pre-existing information obtained about \\(\\vec{\\beta}\\), either through intuition or previous measurements. The term \\(P(\\mathcal{D}|\\mathbf{\\beta})\\) is the likelihood; for fixed \\(\\mathcal{D}\\), let’s denote the likelihood as \\(L(\\vec{\\beta})\\).\nTo find the peak of the posterior distribution (i.e. the maximum a posteriori estimate for \\(\\vec{\\beta}\\)), it is equivalent to minimize the following loss function:\n\\[ \\text{loss} = -\\log L(\\vec{\\beta}) - \\log P(\\vec{\\beta}). \\]\nWe can ignore the \\(P(\\mathcal{D})\\) term because it is constant with respect to \\(\\vec{\\beta}\\). If we take a constant prior, \\(P(\\vec{\\beta}) = 1\\), then the second term on the right hand side can be ignored and we have the standard maximum likelihood approach.\nHowever, there are very good reasons to enforce a non-constant prior on your parameters. For one, we almost always have useful prior knowledge. Consider a simple linear model \\(y \\sim \\vec{\\beta}\\cdot \\vec{x}\\). A standard first step in such an inference problem is to transform \\(\\vec{x}\\) and \\(y\\) such that they have zero mean and unit variance. If we do that, then we can already with fairly high confidence rule out large regions of parameter space. For instance, taking \\(\\beta_i = 100\\) would mean that a unit change in \\(x_i\\) would increase \\(y\\) by 100 standard deviations. This behavior seems unlikely in almost any setting.\nWe can encode this belief in a prior. The prior tells the model to avoid considering regions of parameter space with \\(|\\beta_i| \\gtrsim 100\\), unless there is overwhelming evidence from the data (encoded in the likelihood). This can prevent you from falling into the trap of “overfitting” a model, i.e., seeing a pattern in the data that isn’t really there. The model may say “hey you know if I set \\(\\beta_2 = 100\\), I can explain pretty much everything in the data.” But the prior comes in and says “this conflicts with my expectations, I need more evidence of that before I can believe you.”\nThe devil is, as always, in the details. There are multiple ways to encode the preference for smaller parameters mathematically and which approach you choose turns out to make a big difference in the kind of solutions you obtain. Here, I want to explore two different choices of prior beliefs, both consistent with the preference for smaller parameters, and illustrate how these choices impact the inferred model parameters."
  },
  {
    "objectID": "posts.html",
    "href": "posts.html",
    "title": "",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n  \n\n\n\n\nHyperparameter Optimization and Propensity Score Matching\n\n\n\n\n\n\n\nmachine learning\n\n\ncausal inference\n\n\npropensity score\n\n\n\n\n\n\n\n\n\n\n\nAug 4, 2023\n\n\n11 min\n\n\n\n\n\n\n  \n\n\n\n\nRegularization as a Feature Selection Method\n\n\n\n\n\n\n\nmachine learning\n\n\nlogistic regression\n\n\n\n\n\n\n\n\n\n\n\nJun 7, 2023\n\n\n7 min\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\nNo matching items"
  }
]