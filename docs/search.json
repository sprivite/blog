[
  {
    "objectID": "posts.html",
    "href": "posts.html",
    "title": "",
    "section": "",
    "text": "Order By\n       Default\n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Title\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nKaplan Meier and the Log-Rank Test\n\n\n\n\n\n\nstatistics\n\n\nsurvival analysis\n\n\n\n\n\n\n\n\n\nMar 21, 2024\n\n\n4 min\n\n\n\n\n\n\n\n\n\n\n\n\nHyperparameter Optimization and Propensity Score Matching\n\n\n\n\n\n\nmachine learning\n\n\ncausal inference\n\n\npropensity score\n\n\n\n\n\n\n\n\n\nAug 12, 2023\n\n\n14 min\n\n\n\n\n\n\n\n\n\n\n\n\nRegularization as a Feature Selection Method\n\n\n\n\n\n\nmachine learning\n\n\nlogistic regression\n\n\n\n\n\n\n\n\n\nJun 7, 2023\n\n\n7 min\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "blog/kaplan_meier_log_rank.html",
    "href": "blog/kaplan_meier_log_rank.html",
    "title": "Kaplan Meier and the Log-Rank Test",
    "section": "",
    "text": "The log-rank statistic models random variation in the survival statistics of a population due to finite sampling effects."
  },
  {
    "objectID": "blog/kaplan_meier_log_rank.html#introduction",
    "href": "blog/kaplan_meier_log_rank.html#introduction",
    "title": "Kaplan Meier and the Log-Rank Test",
    "section": "Introduction",
    "text": "Introduction\nThe Kaplan-Meier (KM) method is one way to estimate the survival curve \\(S(t)\\) of a population based on observed event and censoring times. The method is extremely simple – below we will implement the method using numpy in two lines of code. The basic idea is that the probability of survival \\(S(t)\\) at each time point \\(t\\) is a free parameter to be estimated subject to the condition that \\[S(t+\\Delta t) = S(t)P(\\text{survive to }t + \\Delta t|\\text{survive to }t)\\]\nOne question that often comes up when computing KM survival curves is whether two curves are significantly different. For instance, we may have two groups of patients, each group on a different medication, and we want to know if the groups have different chances of survival. The log-rank test is one commonly-applied technique to address this question. In this post, we will give a visual demonstration of this key statistical test."
  },
  {
    "objectID": "blog/kaplan_meier_log_rank.html#problem-setup",
    "href": "blog/kaplan_meier_log_rank.html#problem-setup",
    "title": "Kaplan Meier and the Log-Rank Test",
    "section": "Problem Setup",
    "text": "Problem Setup\nWe start by simulating patient data with a known survival curve. Since KM is a non-parametric method, let’s at least use a survival curve that’s complex enough to not be simply modelled by an exponential. An interesting example is a survival curve in which the initial hazard rate is quite large, but over time the hazard decreases, as illustrated below. This example could be applicable to a medication in which most patients experience severe side effects early on, but over time the risk for severe events decreases.\n\n\nCode\ndef sigmoid(x):\n    return 1./(1 + np.exp(-x))\n\ndef generate_two_phase_hazard(\n    rate_1: float,\n    rate_2: float,\n    transition_time: int, \n    transition_dur: int,\n    observation_time: int):\n    \n    # divide by 2 between the window is symmetric about the transition time\n    transition_dur = transition_dur / 2 \n    \n    time = np.linspace(0, observation_time, observation_time + 1)\n    hazard = \\\n        rate_1 * sigmoid(-(time - transition_time) / transition_dur) + \\\n        rate_2 * sigmoid( (time - transition_time) / transition_dur) \n\n    return hazard\n\ndef compute_survival_from_hazard_rate(hazard_rate):\n    hazard_rate = np.hstack((np.array(0), hazard_rate[:-1]))\n    return np.exp(-hazard_rate.cumsum())\n\nrate_1 = 1./90     # 1 event per 60 days\nrate_2 = 1./180    # 1 event per year\nobservation_time = 365 * 5\ntransition_time = 90\ntransition_dur = 45\nhazard_rate = generate_two_phase_hazard(rate_1, rate_2, transition_time, transition_dur, observation_time)\nsurvival_function = compute_survival_from_hazard_rate(hazard_rate)\nassert np.isclose(survival_function[0], 1)\nsurvival_function = survival_function / survival_function[0]\n\n\n\n\n\n\n\n\n\n\nFigure 1: Theoretical survival curve and hazard rate for toy problem\n\n\n\n\n\nLet’s now simulate 100 patients subject to the above survival curve and see that we can use the Kaplan-Meier approach to estimate the survival curve. In generating these data, we also include censoring, that is, patients which leave the study without experiencing an event. The censoring is completely at random with a constant rate.\n\n\nCode\ndef sample_survival_times(\n    n_patients: int,\n    survival_function: np.array) -&gt; np.array:\n    \n    # sample survival times\n    survival_samples = np.random.rand(n_patients)\n    observation_time = len(survival_function)\n    survival_times = observation_time - np.searchsorted(survival_function[::-1], survival_samples)    \n\n    return survival_times\n    \ndef generate_survival_data(\n    n_patients: int,\n    survival_function: np.array, \n    censor_rate: float) -&gt; pd.DataFrame:\n\n    # require a non-increasing survival function that begins at 1\n    assert survival_function[0] == 1\n    assert np.all(survival_function[:-1] &gt;= survival_function[1:])\n\n    # sample survival times\n    survival_times = sample_survival_times(n_patients, survival_function)\n\n    # apply censoring --\n    # censoring is basically a competing survival process, in this case\n    # with an assumed constant rate\n    time = np.linspace(0, len(survival_function), len(survival_function) + 1)\n    censoring_survival = np.exp(- censor_rate * time)\n    censoring_times = sample_survival_times(n_patients, censoring_survival)\n    \n    patients = list(range(n_patients))\n    time_to_event = np.minimum(survival_times, censoring_times)\n    censoring_flag = np.argmin(np.vstack((survival_times, censoring_times)), axis=0)\n    \n    return pd.DataFrame.from_dict({\n        'patient':patients,\n        'event_time':time_to_event,\n        'is_censored':censoring_flag\n    })    \n\nn_patients = 100\ncensor_rate = 1./180\ncohort_1_events = generate_survival_data(n_patients, survival_function, censor_rate)\ncohort_1_events\n\n\n\n\n\n\n\n\n\n\npatient\nevent_time\nis_censored\n\n\n\n\n0\n0\n155\n0\n\n\n1\n1\n69\n0\n\n\n2\n2\n23\n0\n\n\n3\n3\n8\n0\n\n\n4\n4\n155\n1\n\n\n...\n...\n...\n...\n\n\n95\n95\n40\n1\n\n\n96\n96\n40\n0\n\n\n97\n97\n87\n1\n\n\n98\n98\n10\n0\n\n\n99\n99\n103\n0\n\n\n\n\n100 rows × 3 columns"
  },
  {
    "objectID": "blog/kaplan_meier_log_rank.html#kaplan-meier-curve",
    "href": "blog/kaplan_meier_log_rank.html#kaplan-meier-curve",
    "title": "Kaplan Meier and the Log-Rank Test",
    "section": "Kaplan-Meier Curve",
    "text": "Kaplan-Meier Curve\nIn order to calculate the KM curve, we first aggregate the patient data by event time as follows:\n\n\nCode\ndef aggregate_events(patient_level_events: pd.DataFrame) -&gt; pd.DataFrame:\n\n    n_patients = len(patient_level_events)\n    agg_events = patient_level_events.groupby('event_time').agg(\n        n_events = ('is_censored', lambda x: len(x) - sum(x)),\n        n_censored = ('is_censored', lambda x: sum(x)))\n    agg_events.loc[0] = [0, 0]\n    agg_events = agg_events.sort_values('event_time')\n    agg_events.loc[:, 'at_risk'] = n_patients - \\\n        (agg_events['n_events'] + agg_events['n_censored']).shift(1).fillna(0).cumsum().astype(int)\n\n    return agg_events\n\ncohort_1_events_agg = aggregate_events(cohort_1_events)\ncohort_1_events_agg.loc[:, 'P(t + dt| t)'] = 1 - cohort_1_events_agg['n_events'] / cohort_1_events_agg['at_risk']\ncohort_1_events_agg\n\n\n\n\n\n\n\n\n\n\nn_events\nn_censored\nat_risk\nP(t + dt| t)\n\n\nevent_time\n\n\n\n\n\n\n\n\n0\n0\n0\n100\n1.000000\n\n\n2\n1\n0\n100\n0.990000\n\n\n3\n3\n2\n99\n0.969697\n\n\n4\n1\n0\n94\n0.989362\n\n\n6\n1\n0\n93\n0.989247\n\n\n...\n...\n...\n...\n...\n\n\n217\n0\n1\n5\n1.000000\n\n\n300\n1\n0\n4\n0.750000\n\n\n310\n0\n1\n3\n1.000000\n\n\n332\n1\n0\n2\n0.500000\n\n\n342\n0\n1\n1\n1.000000\n\n\n\n\n71 rows × 4 columns\n\n\n\n\nIn the last column, we added the estimated differential probability of survival in a given interval, computed as\n\\[P(\\text{survive to }t + \\Delta t|\\text{survive to }t) \\approx 1 - n_{events} / n_{at\\_risk}\\]\nNow the calculation of the KM curve is exceedingly simple. The overall survival curve is then given by the cumulative product of the values in this column. The resulting survival curve estimate is shown below.\n\n\nCode\ndef estimate_kaplan_meier_survival(\n    n_events: np.array, \n    at_risk: np.array) -&gt; np.array:\n    \n    km_estimate = 1 - n_events / at_risk\n    km_estimate = km_estimate.cumprod()\n    return km_estimate\n\n\n\n\n\n\n\n\n\n\nFigure 2: Comparison of true survival curve to survival curve estimated with Kaplan-Meier.\n\n\n\n\n\nAs we can see, the estimated curve does not exactly match the theoretical curve. This difference is to be expected due to the finite number of samples we have."
  },
  {
    "objectID": "blog/kaplan_meier_log_rank.html#log-rank-test",
    "href": "blog/kaplan_meier_log_rank.html#log-rank-test",
    "title": "Kaplan Meier and the Log-Rank Test",
    "section": "Log-Rank Test",
    "text": "Log-Rank Test\nTo understand the log-rank test, let’s now consider multiple (independent) samples of 100 patients. By construction, we know the survival curves for the two subgroups are identical, but we will see random fluctuations in the estimated KM curve.\n\n\n\n\n\n\n\n\nFigure 3: Random variations in KM estimate due to finite sampling effects\n\n\n\n\n\n\n\nCode\nfig = plt.figure(figsize=(8,6))\nlr = []\nfor _ in range(100):\n\n    split_agg_1, split_agg_2 = generate_km_curves_random_cohorts(n_patients, survival_function, censor_rate)\n    \n    ax = sns.lineplot(\n        data=split_agg_1[['KM Estimate (1)']], \n        linewidth=0.3,\n        dashes=False,\n        drawstyle='steps-post',\n        palette=sns.color_palette()[1:2],\n        ax=fig.gca()\n    )\n    ax.plot(\n        survival_function, \n        drawstyle='steps-post', \n        linewidth=3, \n        label='True Survival Fxn',\n        color=sns.color_palette()[0]\n    )\n# ax.legend()\nhandles, labels = plt.gca().get_legend_handles_labels()\nby_label = dict(zip(labels, handles))\nplt.legend(by_label.values(), by_label.keys())\nplt.ylabel('Survival probability')\nax.set_ylim([0, 1])\nax.set_xlim([0, 365])\n\n\n\n\n\n\n\n\nFigure 4: An ensemble of random KM curves drawn from the same underlying population.\n\n\n\n\n\nAbove we show an ensemble of 100 random Kaplan-Meier curves drawn from the same underlying population. The plot shows the individual KM curves, each representing survival data from different random splits of the cohort. For comparison, we have drawn also the true survival function, and we see that variations in the estimated KM curve arise out of finite sampling effects. The variation in these estimated curves indicate the expected variation in the KM estimate under the null hypothesis that the the survival curves are the same.\nTo compute the log-rank statistic, we assume that events are distributed randomly between the two groups. For instance, if there are 3 events in a certain time period, 91 patients in group 1 and 90 patients in group 2, then we should expect on average there to be \\(3 * 91 / 181\\) events in group 1 and \\(3 * 90 / 181\\) events in group 2. See the table below.\n\n\nCode\nsplit_agg_1, split_agg_2 = generate_km_curves_random_cohorts(n_patients, survival_function, censor_rate)\ndf_log_rank, lr_stat = compute_log_rank(split_agg_1, split_agg_2)\n\ndf_log_rank[['n_events_1', 'at_risk_1',\n       'n_events_2', 'at_risk_2',\n       'n_events_all', 'at_risk_all', 'n_events_expected_1', 'observed_minus_expected_1']].head(10)\n\n\n\n\n\n\n\n\n\n\nn_events_1\nat_risk_1\nn_events_2\nat_risk_2\nn_events_all\nat_risk_all\nn_events_expected_1\nobserved_minus_expected_1\n\n\nevent_time\n\n\n\n\n\n\n\n\n\n\n\n\n0\n0.0\n100.0\n0.0\n100.0\n0.0\n200.0\n0.000000\n0.000000\n\n\n1\n0.0\n100.0\n0.0\n100.0\n0.0\n200.0\n0.000000\n0.000000\n\n\n2\n2.0\n98.0\n0.0\n98.0\n2.0\n196.0\n1.000000\n-1.000000\n\n\n3\n1.0\n96.0\n0.0\n98.0\n1.0\n194.0\n0.494845\n-0.505155\n\n\n4\n3.0\n95.0\n0.0\n98.0\n3.0\n193.0\n1.476684\n-1.523316\n\n\n5\n6.0\n92.0\n1.0\n98.0\n7.0\n190.0\n3.389474\n-2.610526\n\n\n6\n3.0\n86.0\n1.0\n97.0\n4.0\n183.0\n1.879781\n-1.120219\n\n\n7\n0.0\n83.0\n0.0\n95.0\n0.0\n178.0\n0.000000\n0.000000\n\n\n8\n1.0\n82.0\n1.0\n95.0\n2.0\n177.0\n0.926554\n-0.073446\n\n\n9\n0.0\n81.0\n1.0\n93.0\n1.0\n174.0\n0.465517\n0.465517\n\n\n\n\n\n\n\n\nNow for each of the ensemble pairs generated in the previous plot, we compute the log-rank statistic and plot the histogram of these below. We see that the distribution of the log-rank statistics matches well a \\(\\chi^2\\) distribution with one degree-of-freedom. A high value of the log-rank statistic indicates that it is unlikely for the curves to have come from the same underlying survival distribution.\n\n\nCode\nlr = []\nfor _ in range(1000):\n    split_agg_1, split_agg_2 = generate_km_curves_random_cohorts(n_patients, survival_function, censor_rate)\n    df_log_rank, lr_stat = compute_log_rank(split_agg_1, split_agg_2)\n    lr.append(lr_stat)\n    \nfrom scipy.stats import chi2\nfig = plt.figure(figsize=(8,6))\n_ = plt.hist(lr, bins=np.linspace(0, 5, 31), density=True, label='random 100-100 samples')\nX = 0.05 + np.linspace(0, 5, 32)\ny = [chi2.pdf(x, 1) for x in X]\nplt.plot(X, y, linewidth=3, label='chi2 1-dof')\nplt.xlabel('log-rank statistic')\nplt.ylabel('probability density')\nplt.legend()\n\n\n\n\n\n\n\n\nFigure 5: Distribution of log-rank statistic for random samples of 100 patients having the same underlying survival curve."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Short Stories on Machine Learning and Statistics\n",
    "section": "",
    "text": "These pages are a collection of notes I’ve written to help me remember subtle arguments or how to think about concepts that constantly confuse me. In the hopes that maybe my way of thinking resonates with others, I’ve decided to share these notes here.\nPlease feel free to reach out if you want to connect on these topics."
  },
  {
    "objectID": "blog/hyperparameter_optimization.html",
    "href": "blog/hyperparameter_optimization.html",
    "title": "Hyperparameter Optimization and Propensity Score Matching",
    "section": "",
    "text": "Despite the intimidating names given to methods like tree-structured Parzen estimation, hyperparameter optimization algorithms are extremely intuitive and easy to understand."
  },
  {
    "objectID": "blog/hyperparameter_optimization.html#introduction",
    "href": "blog/hyperparameter_optimization.html#introduction",
    "title": "Hyperparameter Optimization and Propensity Score Matching",
    "section": "Introduction",
    "text": "Introduction\n\nHyperparameter optimization\nThis post came about as a rebellion against obfuscation. I was trying to optimize the hyperparameters of a classification model and came across an interesting-looking paper [1]. Unfortunately, the paper is dense with mathematics and gave me no intuitive insight into what’s happening. As I dug deeper, I found that the ideas they were presenting are absurdly simple (thanks partly to the post in [2]) and I wanted to capture my own take on this simplicity for future reference.\nI will do this in the actual setting in which my use case arose: propensity score (PS) matching. There are some prerequisites for this discussion, which I unfortunately have to assume. If you aren’t familiar, the classic reference on PS matching is [3], although I find the more recent review in [4] to be more accessible. In any case, suffice it to say that in PS matching, one trains a binary classification model and needs somehow to choose the “best” possible model over a large space of modeling choices.\nI refer to these modeling choices as hyperparameters. For me, a hyperparameter is any decision you make along the way to making a prediction with a model that cannot be optimized by gradient descent. I like this definition because it includes preprocessing operations that sometimes aren’t thought of as hyperparameters. For instance, do you subtract the mean from your variables before optimizing the model parameters? This choice is a hyperparameter: subtract_mean \\(\\in \\{True, False\\}\\). In many cases, there is conventional wisdom around certain choices and these choices have a well-chosen default, but the choice is there, it affects your analysis, it can’t be optimized by gradient descent and it’s therefore by my definition a hyperparameter.\nMore conventional hyperparameters are quantities like regularization strength (which was discussed in a previous post ), type of regularization (e.g., L1 versus L2, discussed in the same post), maximum tree depth for a tree-based model, number of estimators in an ensemble method, number of layers in a multi-layer perceptron, and even the choice of which model to use. A bad choice for hyperparameters can lead to poor performance in your model, so it’s worthwhile to spend time thinking about how one can best optimize an objective function over these hyperparameters.\nIn this post, we will motivate the need for hyperparameter optimization and give a very high level explanation of how two popular hyperparameter optimization approaches (tree-structured Parzen estimation and Gaussian process regression) work. The main goal is to break through the jargon and emphasize the intuition behind these methods. There will be minimal mathematics and we will not be rigorous! We will then demonstrate tree-structured Parzen estimation in action applied to PS hyperparameter optimization.\n\n\nPropensity score matching\nWe want to answer the question: does treatment X cause outcome Y? The ideal approach to answering such a question is the randomized control trial; however, it’s often very impractical to perform such a trial. Meanwhile, there exists a wealth of high quality data that potentially already contain the cause-effect signal we’re interested in, for instance, electronic health record data generated through routine medical practice.\nWhen the intervention is not randomly assigned, there will often be systematic differences between treatment groups, which may obscure the causal signal if not taken into account. It turns out that by “matching” patients on their probability of treatment assignment, referred to as their propensity score, one can generate groups of patients with identical distributions of baseline covariates. It follows then that one can compare outcomes in these matched groups and obtain an unbiased estimate of the treatment effect. For details, see [3] and [4].\nThis all works in theory, when the propensity score is known. However, the propensity score is never known in observational settings and must be estimated from the data. The estimation of the propensity score from the data requires modeling choices and we want to choose the “best” propensity score model over a large space of possible models. Here, we diverge a bit from traditional machine learning practices. Since the end goal is balance in the baseline covariates, we use this as an optimization target. Whereas traditionally one would use an “out-of-sample” measure of model performance, here out-of-sample behavior is not relevant.\nWe define the following procedure for optimizing the hyperparameters of the propensity score model: \n\nChoose a set of hyperparameters \nTrain a model to predict P(treat|covariates) \nPerform matching on the estimated propensity score \nEvaluate the achieved covariate balance \n\nOur focus will be on how to implement step (1) such that we arrive quickly at a good set of hyperparameters, since steps (2) and (3) can be quite time-consuming. For (4), we compute the area between the 1-dimensional marginal distributions as a measure of balance to be optimized."
  },
  {
    "objectID": "blog/hyperparameter_optimization.html#simulation-setup",
    "href": "blog/hyperparameter_optimization.html#simulation-setup",
    "title": "Hyperparameter Optimization and Propensity Score Matching",
    "section": "Simulation Setup",
    "text": "Simulation Setup\nTo start, we’ll simulate a toy dataset consisting of two covariates. To make the example more tangible, we’ll give the covariates names. One will be BMI, the other will be eGFR. We’re interested in studying the effect of a certain medication on a patient population as seen in electronic health record databases. The problem: both BMI and eGFR influence the treatment decision by practicing physicians. The medication is not prescribed to patients that are extremely overweight, nor to patients that are extremely underweight. Furthermore, the medication is not prescribed to patients with poor kidney function (low eGFR), nor is it prescribed to patients with normal kidney function (high eGFR). As a result of the interaction between these covariates and the treatment decision, we cannot simply compared treated and untreated patients. We have to correct for these systematic differences through matching.\nBelow we show the distribution of patients and the simulated propensity score. The propensity score shown here represents the “true” (unknown) propensity score that generates the treatment decision. Our goal here is to estimate this quantity.\n\n\nCode\ndef make_bmi_egfr_blob(n_samples: int) -&gt; pd.DataFrame:\n    \n    # We mode eGFR to have a mean of 55 and a spread of +/- 40\n    # and clip to range [5, 120]. This clipping is actually common\n    # as often measurements just report &gt;= 120.\n    egfr = 55 + 40 * np.random.power(1, n_samples) * np.random.choice([-1, 1], size=n_samples)\n    egfr = np.clip(egfr, 5, 120)\n    \n    # We model BMI to have mean 20 and a spread +/- 10 and\n    # clip to range [2, 80].\n    bmi = 20 + 10 * np.random.randn(n_samples)\n    bmi = np.clip(bmi, 2, 80)\n\n    features = pd.DataFrame.from_records(\n        np.vstack((egfr, bmi)).T,\n        columns=['eGFR', 'BMI']\n    )\n    \n    return features\n\ndef ps_true(x: np.ndarray, y: np.ndarray) -&gt; np.ndarray:\n    # propensity score is a circle of radius == standard deviation around the mean\n    x_ = (x - x.mean()) / x.std()\n    y_ = (y - y.mean()) / y.std()\n    return expit( - (x_**2 + y_**2 ) )\n\n\n\n\n\n\n\n\n\n\nFigure 1: Scatter plot showing the BMI and eGFR values of the simulated patient population. Color indicates the true (and unknown) probability of treatment.\n\n\n\n\n\nGiven this assumed form for the true propensity score (i.e., the true probability of treatment assignment), we can now sample the realized treatment group assignment. In doing so, we get the distributions shown below for BMI and eGFR for the treated and untreated groups. We see that there are significant differences between the treated and untreated group other than the treatment. It would therefore not be fair to directly compare the health outcomes of the groups and then attribute differences in outcomes to the treatment.\n\n\nCode\ntreat_indicator = (np.random.rand(n_samples) &lt;= ps_true).astype(int)\ntreat_label = ['treated' if t else 'untreated' for t in treat_indicator]\n\n\n\n\n\n\n\n\n\n\nFigure 2: Simulated input distributions to the matching problem.\n\n\n\n\n\nThe number of patients in each group is given as below. The goal will be to choose a subset of patients from the untreated group such that distributions of BMI and eGFR in the chosen subset match the distributions in the treated group.\n\n\n\n\n\n\n\n\n\n\nN\n\n\ntreat\n\n\n\n\n\nuntreated\n8024\n\n\ntreated\n1976"
  },
  {
    "objectID": "blog/hyperparameter_optimization.html#matching-on-the-true-propensity-score",
    "href": "blog/hyperparameter_optimization.html#matching-on-the-true-propensity-score",
    "title": "Hyperparameter Optimization and Propensity Score Matching",
    "section": "Matching on the true propensity score",
    "text": "Matching on the true propensity score\nIn practice, we never have direct knowledge of the propensity score. But since we do in our simulation, let’s first check what happens when we match using the true propensity score. Namely, for each patient in the treated group, we select a corresponding patient from the untreated group with a similar propensity score. The resulting population is denoted by the label “matched” in the figure below. This solution represents the ideal case, the best we can hope to achieve with propensity score matching.\n\n\n\n\n\n\n\n\nFigure 3: Result of matching using the true propensity score.\n\n\n\n\n\nAs you can see, propensity score matching delivers on its promise – the covariates distributions of the population matched to the treated population on the propensity score (green) are nearly identical to that of the treated population (blue)."
  },
  {
    "objectID": "blog/hyperparameter_optimization.html#matching-on-estimated-propensity-score",
    "href": "blog/hyperparameter_optimization.html#matching-on-estimated-propensity-score",
    "title": "Hyperparameter Optimization and Propensity Score Matching",
    "section": "Matching on estimated propensity score",
    "text": "Matching on estimated propensity score\nNow let’s see how well we can perform matching by estimating the propensity score. To start, let’s try using a logistic regression to estimate the propensity score. As the logistic regression has linear decision boundaries, it should be clear that such a model would not be expected to perform well on this particular data, and indeed that is what we see.\n\n\nCode\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nnormalized_features = scaler.fit_transform(model_features[['BMI', 'eGFR']])\nclf = LogisticRegression()\nclf.fit(normalized_features, treat_indicator)\nps_est = clf.predict_proba(normalized_features)[:, 1]\nps_true_ps_est_plot(ps_true, ps_est)\n\n\n\n\n\n\n\n\nFigure 4: Result of using logistic regression to model propensity score.\n\n\n\n\n\nAs seen in the left hand plot above, the model estimates the true PS very poorly. As a result, when we match using the estimated PS, we get very poor balance, as seen below. Although the distribution on BMI looks somewhat balanced, eGFR in the matched population is left completely unbalanced. This result is simply a consequence of our modeling choice, not a breakdown of the PS approach, since we know from above that matching works when we know the true PS.\n\n\n\n\n\n\n\n\nFigure 5: Achieved balance of PS matching using logistic regression model for PS.\n\n\n\n\n\nSince it’s clear that the propensity score has some non-linearity to it, it make sense to next try using some sort of model that can capture this non-linearity. Let’s try therefore a random forest model for the propensity score. To illustrate the point that bad hyperparameters can impact performance, I will purposely pick very poor hyperparameters for the random forest: max_depth=2, n_estimators=2 (for this toy problem, the default hyperparameters perform quite well, but in realistic problems this may not be the case). The resulting estimation for PS is shown below. Although the estimation is still quite poor, the random forest does seem to better model the PS than the logistic regression for these particular choices of hyperparameters.\n\n\nCode\nfrom sklearn.ensemble import RandomForestClassifier\nnp.random.seed(124)\nclf = RandomForestClassifier(max_depth=2, n_estimators=2, random_state=123)\nclf.fit(normalized_features, treat_indicator)\nps_est = clf.predict_proba(normalized_features)[:, 1]\nps_true_ps_est_plot(ps_true, ps_est)\n\n\n\n\n\n\n\n\nFigure 6: Result of using random forest to model propensity score.\n\n\n\n\n\nThe poor performance of this choice of hyperparameters is also reflected in the resulting matched distributions, shown below. Although better than the logistic regression, it’s clear that the matching is far from the ideal case of matching on the true PS. So we have some work left.\n\n\n\n\n\n\n\n\nFigure 7: Achieved balance of PS matching using random forest model for PS."
  },
  {
    "objectID": "blog/hyperparameter_optimization.html#hyperparameter-optimization-1",
    "href": "blog/hyperparameter_optimization.html#hyperparameter-optimization-1",
    "title": "Hyperparameter Optimization and Propensity Score Matching",
    "section": "Hyperparameter optimization",
    "text": "Hyperparameter optimization\n\nMotivation and intuition\nWe’ve seen that the choice of hyperparameters makes a difference in the achieved balance. So let’s try a few more choices of hyperparameters and see if we can find some improvements in the balance. For the logistic regression, we randomly sample a few values for the regularization strength \\(C\\); for the random forest model, we randomly sample a few values for the max_depth, keeping the n_estimators fixed to 2, as before. The resulting balance (roughly, the total area between the 1D marginal distributions) as a function of these hyperparameters is shown below.\n\n\n\n\n\n\n\n\nFigure 8: Balance achieved by PS matching using various models for PS.\n\n\n\n\n\nAfter trying a few values of hyperparameters, we’ve realized that training models takes a long time and we can only try a limited number of hyperparameters. Now that we have some information about how the achieved balance depends on our hyperparameters, it would make sense to use this information to choose the next point! If given only one more chance, which model would you pick? So far it looks like the random forest model is the better bet and furthermore, it might be useful to try a max depth between 5 and 15, as this seems to be the region of hyperparameter space where the balance is the best.\nThis simple calculation, which you just did in your brain instinctively, is really all there is to hyperparameter optimization by tree-structured Parzen estimation (TPE) or Gaussian process regression (GPR), at least conceptually. For each model (logistic regression or random forest), we construct a “hypermodel” which predicts the expected balance on unseen values of the hyperparameters. We then use these hypermodels to guess a good set of hyperparameters to test next. The various methods refer simply to different ways of predicting what values of balance lie in between the sampled points and methods for choosing the next point to sample.\nAs an illustration, we show the result of fitting a Gaussian process to the sampled data points. We perform one GPR per model class (logistic regression or random forest). The plot below shows the mean +/- 1 standard deviation range for the predicted values on the intermediate points. We see that the Gaussian process is simply interpolating on the known points and giving additionally a measure of uncertainty in the interpolated values.\n\n\nCode\nfrom sklearn.gaussian_process import GaussianProcessRegressor\nfrom sklearn.gaussian_process.kernels import RBF\n\ngp_lr = GaussianProcessRegressor(normalize_y=True)\n_ = gp_lr.fit(np.log([C]).T, balance_lr)\n\ngp_rf = GaussianProcessRegressor(normalize_y=True, kernel=RBF(length_scale_bounds=(1, 100)))\n_ = gp_rf.fit(np.array([max_depth]).T, balance_rf)\n\n\n\n\n\n\n\n\n\n\nFigure 9: Gaussian process fit to achieved balance as a function of the hyperparameters for each model type.\n\n\n\n\n\nTPE works in a similar way, except instead of directly modeling \\(P\\)(balance|hyperparameters) as the GPR does, TPE models the distributions of “good” and “bad” points. That is, TPE splits all the sampled points into two categories based on some arbitrary threshold \\(B^*\\). Those points with balance \\(&gt; B^*\\) are considered “bad” and those with balance \\(\\leq B^*\\) are considered “good”. We then form a KDE (kernel density estimate – essentially, a fancy kind of histogram) for the good and bad points. And we do this for each model class. From these distributions, we sample the next hyperparameter according to the ratio \\(P\\)(good|hyperparameter) / \\(P\\)(bad|hyperparameter). The resulting sampling distribution is shown below for \\(B^*=0.125\\). As we can see in the right-hand plot, the TPE approach seems to encode our intuition that sampling a point max_depth \\(\\in\\) [5, 15] next is a good idea. The choice of whether to sample from the logistic regression or random forest model hyperparameters is handled by yet another hypermodel, not shown.\n\n\nCode\nfrom sklearn.neighbors import KernelDensity\n\nBstar = 0.125\n\ngood_lr = [[c for c,b in zip(C, balance_lr) if b &lt; Bstar]]\nbad_lr = [[c for c,b in zip(C, balance_lr) if b &gt;= Bstar]]\ngood_rf = [[d for d,b in zip(max_depth, balance_rf) if b &lt; Bstar]] \nbad_rf = [[d for d,b in zip(max_depth, balance_rf) if b &gt;= Bstar]]\n\nbandwidth = 2\nkernel = 'gaussian'\nkde = KernelDensity(bandwidth=bandwidth, kernel=kernel)\ngood_lr_kde = kde.fit(np.log(good_lr).T)\n\nkde = KernelDensity(bandwidth=bandwidth, kernel=kernel)\nbad_lr_kde = kde.fit(np.log(bad_lr).T)\n\nbandwidth = 5\nkde = KernelDensity(bandwidth=bandwidth, kernel=kernel)\ngood_rf_kde = kde.fit(np.array(good_rf).T)\n\nkde = KernelDensity(bandwidth=bandwidth, kernel=kernel)\nbad_rf_kde = kde.fit(np.array(bad_rf).T)\n\n\n\n\n\n\n\n\n\n\nFigure 10: Sampling distribution obtained by fitting Gaussian KDEs to the observed values of the balance for each model type.\n\n\n\n\n\n\n\nTree-structured Parzen estimator in action\nSo, now that we understand what the “fancy” hyperparameter optimization methods are doing at a very high level, let’s see if we can use them to achieve even better balance. We will use the implementation of TPE from the hyperopt library. To make things a little more interesting, we’ll optimize not just over model choice (LogisticRegression or RandomForest), \\(C\\) and max_depth, but also over the penalty type (L1 or L2) for logistic regression, n_estimators and min_samples_leaf for random forest (see here for a description of these parameters). After running the TPE sampler for 1000 iterations, the code returns the best parameters found, shown below.\n\n\nCode\n# define a search space\nfrom hyperopt import hp\nfrom hyperopt import fmin, tpe, space_eval, Trials\n\nspace = hp.choice('model', [\n    (LogisticRegression, {\n        'C':hp.loguniform('C', -4.6, 4.6), # BASE E\n        'penalty':hp.choice('penalty', ['l1', 'l2']),\n        'solver':'saga'\n    }),\n    (RandomForestClassifier, {\n        'n_estimators':hp.uniformint('n_estimators', 2, 100),\n        'min_samples_leaf':hp.uniformint('min_samples_leaf', 1, 500),\n        'max_depth':hp.uniformint('max_depth', 2, 30),\n        'random_state':123\n    })\n])\n\n# minimize the objective over the space\ntrials = Trials()\n# np.random.seed(2345); The random seed is set above, if the notebook is run\n# from the beginning, one gets reproducible results\nbest = fmin(ps_train_and_match, space, algo=tpe.suggest, max_evals=1000, trials=trials, verbose=False)\n\nprint('Best model found:')\nfor param in best.items():\n    print(f'\\t{param[0]:20}{param[1]}')\n    \n\n\nBest model found:\n    max_depth           12.0\n    min_samples_leaf    223.0\n    model               1\n    n_estimators        11.0\n\n\nLet’s take a look at what the sampler did to arrive at this result. Below, I show how the sampling of the hyperparameter space evolves over time using the TPE method.\n\n\n\n\n\n\n\n\nFigure 11: An illustration of how the sampling of hyperparameters varies over time in the TPE approach. In the bottom panel, the black line indicates the average balance over the last 50 samples and demonstrates how the TPE method is learning over time to suggest better and better hyperparameters.\n\n\n\n\n\nWe see a number of interesting features. In the top frame, we see that the sampler choose the logistic regression (LR) model less frequently over time; this is consistent with what we saw above, namely that the random forest model seems to perform better. The hypermodels of the TPE pick up on this fact and use it to preferentially choose the model that is showing the best results.\nWe also see that the sampler explores the space rather uniformly in the first ~50-100 iterations. The sampler is doing this because it needs something to train its hypermodels on. Once it has some data to work with, the sampler can start taking more educated guesses about where to explore next. We see this in the time series of the samples for max_depth, n_estimators and min_samples_leaf. For later times, the sampler almost never chooses max_depth outside the range [5, 20]. It seems to have figured out that such values doesn’t have a good chance of working out. Similarly, the sampler stops choosing n_estimators &gt; 25 and zeros in on the range [100, 300] for min_samples_leaf after a while.\nNote that the sampler never completely stops exploring any region of parameter space. Especially as the number of hyperparameter dimensions grows large, there will always be unexplored regions of parameter space. Therefore, this occasional random exploration of the space, even when a good objective value seems unlikely, is useful to avoid missing out.\nFinally, let’s see what the matching results look like! Below, I show the resulting PS estimation using the best values for the hyperparameters found by the TPE sampler. Although the results are not perfect, we have clearly greatly improved upon the initial results.\n\n\n\n\n\n\n\n\nFigure 12: Result of using TPE to find best hyperparameters to model propensity score.\n\n\n\n\n\nFurthermore, as shown below, the resulting matched populations look great! I would argue this match is hard to distinguish from the match above using the true propensity score.\n\n\n\n\n\n\n\n\nFigure 13: Achieved balance of PS matching using best model achieved from TPE hyperparameter optimization."
  },
  {
    "objectID": "blog/hyperparameter_optimization.html#conclusion",
    "href": "blog/hyperparameter_optimization.html#conclusion",
    "title": "Hyperparameter Optimization and Propensity Score Matching",
    "section": "Conclusion",
    "text": "Conclusion\nHyperparameter optimzation is an important step in any machine learning workflow. Random sampling of hyperparameters is a very inefficient way to optimize hyperparameters, as our eyes can very quickly tell us. Tree-structured Parzen estimation and Gaussian process regression are a nice alternatives to random hyperparameter search, with easy-to-use Python implementations readily available. Although the methods sound intimidating, they are not much more than a formalization of what we would naturally do anyway if we were to manually choose hyperparameters to test."
  },
  {
    "objectID": "blog/hyperparameter_optimization.html#further-reading",
    "href": "blog/hyperparameter_optimization.html#further-reading",
    "title": "Hyperparameter Optimization and Propensity Score Matching",
    "section": "Further Reading",
    "text": "Further Reading\n[1] Algorithms for Hyper-Parameter Optimization.\n[2] Building a Tree-Structured Parzen Estimator from Scratch(Kind Of).\n[3] The Central Role of the Propensity Score in Observational Studies for Causal Effects.\n[4] An Introduction to Propensity Score Methods for Reducing the Effects of Confounding in Observational Studies.\n[5] Hyperopt (Python library)."
  },
  {
    "objectID": "blog/regularization.html",
    "href": "blog/regularization.html",
    "title": "Regularization as a Feature Selection Method",
    "section": "",
    "text": "Often thought of as a method to prevent overfitting, regularization can also be used to select the most important features from a model."
  },
  {
    "objectID": "blog/regularization.html#what-is-regularization",
    "href": "blog/regularization.html#what-is-regularization",
    "title": "Regularization as a Feature Selection Method",
    "section": "What is regularization?",
    "text": "What is regularization?\nIn any machine learning setting, you invariably find yourself with a set of parameters \\(\\vec{\\beta}\\), which you wish to estimate from the data \\(\\mathcal{D}\\). A standard approach to estimating \\(\\vec{\\beta}\\) is to write down a likelihood function \\(P(\\mathcal{D}|\\mathbf{\\beta})\\) and maximize the likelihood with respect to \\(\\vec{\\beta}\\). This approach is called maximum likelihood estimation.\nWith regularization, we shift from a maximum likelihood estimation to a maximum a posterior estimation. In the Bayes’ formulation, we have the posterior equation:\n\\[ P(\\vec{\\beta} | \\mathcal{D}) = \\frac{P(\\mathcal{D}|\\vec{\\beta}) P(\\vec{\\beta})}{P(\\mathcal{D})}, \\]\nwhere the term \\(P(\\vec{\\beta})\\) is called the prior. It represents all pre-existing information obtained about \\(\\vec{\\beta}\\), either through intuition or previous measurements. The term \\(P(\\mathcal{D}|\\mathbf{\\beta})\\) is the likelihood; for fixed \\(\\mathcal{D}\\), let’s denote the likelihood as \\(L(\\vec{\\beta})\\).\nTo find the peak of the posterior distribution (i.e. the maximum a posteriori estimate for \\(\\vec{\\beta}\\)), it is equivalent to minimize the following loss function:\n\\[ \\text{loss} = -\\log L(\\vec{\\beta}) - \\log P(\\vec{\\beta}). \\]\nWe can ignore the \\(P(\\mathcal{D})\\) term because it is constant with respect to \\(\\vec{\\beta}\\). If we take a constant prior, \\(P(\\vec{\\beta}) = 1\\), then the second term on the right hand side can be ignored and we have the standard maximum likelihood approach.\nHowever, there are very good reasons to enforce a non-constant prior on your parameters. For one, we almost always have useful prior knowledge. Consider a simple linear model \\(y \\sim \\vec{\\beta}\\cdot \\vec{x}\\). A standard first step in such an inference problem is to transform \\(\\vec{x}\\) and \\(y\\) such that they have zero mean and unit variance. If we do that, then we can already with fairly high confidence rule out large regions of parameter space. For instance, taking \\(\\beta_i = 100\\) would mean that a unit change in \\(x_i\\) would increase \\(y\\) by 100 standard deviations. This behavior seems unlikely in almost any setting.\nWe can encode this belief in a prior. The prior tells the model to avoid considering regions of parameter space with \\(|\\beta_i| \\gtrsim 100\\), unless there is overwhelming evidence from the data (encoded in the likelihood). This can prevent you from falling into the trap of “overfitting” a model, i.e., seeing a pattern in the data that isn’t really there. The model may say “hey you know if I set \\(\\beta_2 = 100\\), I can explain pretty much everything in the data.” But the prior comes in and says “this conflicts with my expectations, I need more evidence of that before I can believe you.”\nThe devil is, as always, in the details. There are multiple ways to encode the preference for smaller parameters mathematically and which approach you choose turns out to make a big difference in the kind of solutions you obtain. Here, I want to explore two different choices of prior beliefs, both consistent with the preference for smaller parameters, and illustrate how these choices impact the inferred model parameters."
  }
]